{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73446c8",
   "metadata": {},
   "source": [
    "Showing all the Urls present in the SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests # pip install requests #to sent GET requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup # pip install bs4 #to parse html(getting data out from html, xml or other markup languages)\n",
    "\n",
    "# user can input a search keyword and the count of images required\n",
    "# download images from google search image\n",
    "Google_Image = \\\n",
    "    'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\n",
    "# https://www.amazon.in/Pavilion-Multitouch-Windows-Graphics-14-Dy0207Tu/dp/B09VT6BVJF\n",
    "# The User-Agent request header contains a characteristic string \n",
    "# that allows the network protocol peers to identify the application type, \n",
    "# operating system, and software version of the requesting software user agent.\n",
    "# needed for google search\n",
    "u_agnt = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "    'Accept-Encoding': 'none',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Connection': 'keep-alive',\n",
    "} #write: 'my user agent' in browser to get your browser user agent details\n",
    "\n",
    "Image_Folder = 'Images_1'\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(Image_Folder):\n",
    "        os.mkdir(Image_Folder)\n",
    "    download_images()\n",
    "\n",
    "def download_images():\n",
    "    data = input('Enter your search keyword: ')\n",
    "    num_images = int(input('Enter the number of images you want: '))\n",
    "    \n",
    "    print('Searching Images....')\n",
    "    \n",
    "    search_url = Google_Image + 'q=' + data #'q=' because its a query\n",
    "    \n",
    "    # request url, without u_agnt the permission gets denied\n",
    "    response = requests.get(search_url, headers=u_agnt)\n",
    "    html = response.text #To get actual result i.e. to read the html data in text mode\n",
    "    \n",
    "    # find all img where class='rg_i Q4LuWd'\n",
    "    b_soup = BeautifulSoup(html, 'html.parser') #html.parser is used to parse/extract features from HTML files\n",
    "    results = b_soup.findAll('img', {'class': 'rg_i Q4LuWd'})\n",
    "    \n",
    "    #extract the links of requested number of images with 'data-src' attribute and appended those links to a list 'imagelinks'\n",
    "    #allow to continue the loop in case query fails for non-data-src attributes\n",
    "    count = 0\n",
    "    imagelinks= []\n",
    "    urls=[]\n",
    "    for res in results:\n",
    "        try:\n",
    "            link = res['data-src']\n",
    "            imagelinks.append(link)\n",
    "            urls.append(link)\n",
    "            count = count + 1\n",
    "            if (count >= num_images):\n",
    "                break\n",
    "            \n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    print(f'Found {len(imagelinks)} images')\n",
    "    print('Start downloading...')\n",
    "\n",
    "    for i, imagelink in enumerate(imagelinks):\n",
    "        # open each image link and save the file\n",
    "        response = requests.get(imagelink)\n",
    "        imagename = Image_Folder + '/' + data + str(i+1) + '.jpg'\n",
    "        with open(imagename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            \n",
    "    # Name of the SQL Database is image_urls        \n",
    "    conn=sqlite3.connect('image_urls.db')\n",
    "    cursor=conn.cursor()\n",
    "    cursor.execute('CREATE TABLE IF NOT EXISTS image_urls (id INTEGER PRIMARY KEY AUTOINCREMENT, url TEXT)')\n",
    "    \n",
    "    for url in urls:\n",
    "        cursor.execute('INSERT INTO image_urls (url) VALUES (?)', (url,))\n",
    "    \n",
    "    cursor.execute('SELECT * FROM image_urls')\n",
    "    rows=cursor.fetchall()\n",
    "    \n",
    "    for row in rows:\n",
    "        print(row[1])\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print('Download Completed!')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fadabe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search keyword: cats\n",
      "Enter the number of images you want: 1\n",
      "Searching Images....\n",
      "Found 0 images\n",
      "Start downloading...\n",
      "Download Completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests # pip install requests #to sent GET requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup # pip install bs4 #to parse html(getting data out from html, xml or other markup languages)\n",
    "\n",
    "# user can input a search keyword and the count of images required\n",
    "# download images from google search image\n",
    "Google_Image = 'https://www.amazon.in/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\n",
    "\n",
    "# The User-Agent request header contains a characteristic string \n",
    "# that allows the network protocol peers to identify the application type, \n",
    "# operating system, and software version of the requesting software user agent.\n",
    "# needed for google search\n",
    "u_agnt = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "    'Accept-Encoding': 'none',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Connection': 'keep-alive',\n",
    "} #write: 'my user agent' in browser to get your browser user agent details\n",
    "\n",
    "Image_Folder = 'Images_1'\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(Image_Folder):\n",
    "        os.mkdir(Image_Folder)\n",
    "    download_images()\n",
    "\n",
    "def download_images():\n",
    "    data = input('Enter your search keyword: ')\n",
    "    num_images = int(input('Enter the number of images you want: '))\n",
    "    \n",
    "    print('Searching Images....')\n",
    "    \n",
    "    search_url = 'https://www.amazon.in/Pavilion-Multitouch-Windows-Graphics-14-Dy0207Tu/dp/B09VT6BVJF'\n",
    "    #'q=' because its a query\n",
    "    \n",
    "    # request url, without u_agnt the permission gets denied\n",
    "    response = requests.get(search_url, headers=u_agnt)\n",
    "    html = response.text #To get actual result i.e. to read the html data in text mode\n",
    "    \n",
    "    # find all img where class='rg_i Q4LuWd'\n",
    "    b_soup = BeautifulSoup(html, 'html.parser') #html.parser is used to parse/extract features from HTML files\n",
    "    results = b_soup.findAll('img', {'class': 'rg_i Q4LuWd'})\n",
    "    \n",
    "    #extract the links of requested number of images with 'data-src' attribute and appended those links to a list 'imagelinks'\n",
    "    #allow to continue the loop in case query fails for non-data-src attributes\n",
    "    count = 0\n",
    "    imagelinks= []\n",
    "    urls=[]\n",
    "    for res in results:\n",
    "        try:\n",
    "            link = res['data-src']\n",
    "            imagelinks.append(link)\n",
    "            urls.append(link)\n",
    "            count = count + 1\n",
    "            if (count >= num_images):\n",
    "                break\n",
    "            \n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    print(f'Found {len(imagelinks)} images')\n",
    "    print('Start downloading...')\n",
    "\n",
    "    for i, imagelink in enumerate(imagelinks):\n",
    "        # open each image link and save the file\n",
    "        response = requests.get(imagelink)\n",
    "        imagename = Image_Folder + '/' + data + str(i+1) + '.jpg'\n",
    "        with open(imagename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            \n",
    "    for url in urls:\n",
    "        print(url)\n",
    "    \n",
    "    print('Download Completed!')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f60cc",
   "metadata": {},
   "source": [
    "Showing only the urls search for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab12bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search keyword: Google\n",
      "Enter the number of images you want: 4\n",
      "Searching Images....\n",
      "Found 4 images\n",
      "Start downloading...\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQadi52TacfX8PB_HrIuORgBbNk-YTIOsBq4g&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS2D_hJ0NOy0i1MTZt2rN3cfVwk1a6Q0etGSQ&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQFRLBp9HZDDm3MP53zRVFAN0X9d_W0oaKnNg&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRgCEnkrdmMzgYiZGkdAug7nyEZ39_dP03-SA&usqp=CAU\n",
      "Download Completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests # pip install requests #to sent GET requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup # pip install bs4 #to parse html(getting data out from html, xml or other markup languages)\n",
    "\n",
    "# user can input a search keyword and the count of images required\n",
    "# download images from google search image\n",
    "Google_Image = \\\n",
    "    'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\n",
    "\n",
    "# The User-Agent request header contains a characteristic string \n",
    "# that allows the network protocol peers to identify the application type, \n",
    "# operating system, and software version of the requesting software user agent.\n",
    "# needed for google search\n",
    "u_agnt = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "    'Accept-Encoding': 'none',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Connection': 'keep-alive',\n",
    "} #write: 'my user agent' in browser to get your browser user agent details\n",
    "\n",
    "Image_Folder = 'Images_1'\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(Image_Folder):\n",
    "        os.mkdir(Image_Folder)\n",
    "    download_images()\n",
    "\n",
    "def download_images():\n",
    "    data = input('Enter your search keyword: ')\n",
    "    num_images = int(input('Enter the number of images you want: '))\n",
    "    \n",
    "    print('Searching Images....')\n",
    "    \n",
    "    search_url = Google_Image + 'q=' + data #'q=' because its a query\n",
    "    \n",
    "    # request url, without u_agnt the permission gets denied\n",
    "    response = requests.get(search_url, headers=u_agnt)\n",
    "    html = response.text #To get actual result i.e. to read the html data in text mode\n",
    "    \n",
    "    # find all img where class='rg_i Q4LuWd'\n",
    "    b_soup = BeautifulSoup(html, 'html.parser') #html.parser is used to parse/extract features from HTML files\n",
    "    results = b_soup.findAll('img', {'class': 'rg_i Q4LuWd'})\n",
    "    \n",
    "    #extract the links of requested number of images with 'data-src' attribute and appended those links to a list 'imagelinks'\n",
    "    #allow to continue the loop in case query fails for non-data-src attributes\n",
    "    count = 0\n",
    "    imagelinks= []\n",
    "    urls=[]\n",
    "    for res in results:\n",
    "        try:\n",
    "            link = res['data-src']\n",
    "            imagelinks.append(link)\n",
    "            urls.append(link)\n",
    "            count = count + 1\n",
    "            if (count >= num_images):\n",
    "                break\n",
    "            \n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    print(f'Found {len(imagelinks)} images')\n",
    "    print('Start downloading...')\n",
    "\n",
    "    for i, imagelink in enumerate(imagelinks):\n",
    "        # open each image link and save the file\n",
    "        response = requests.get(imagelink)\n",
    "        imagename = Image_Folder + '/' + data + str(i+1) + '.jpg'\n",
    "        with open(imagename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            \n",
    "    for url in urls:\n",
    "        print(url)\n",
    "    \n",
    "    print('Download Completed!')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "049ead8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search keyword: Agra\n",
      "Enter the number of images you want: 4\n",
      "Searching Images....\n",
      "Found 4 images\n",
      "Start downloading...\n",
      "(46, 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTzt0kPd8TG-k1FaY99vqiKlS_Ka7FK630_Ug&usqp=CAU')\n",
      "(23, 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTrcuqZmJLAAQbQ8UjH3cNLNq8Iy_-yyqGiog&usqp=CAU')\n",
      "(7, 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTdr0NjhNJx9kVq8tj4TeCN-_s_wnEvqV9NVg&usqp=CAU')\n",
      "(11, 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTdr0NjhNJx9kVq8tj4TeCN-_s_wnEvqV9NVg&usqp=CAU')\n",
      "Download Completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests # pip install requests #to sent GET requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup # pip install bs4 #to parse html(getting data out from html, xml or other markup languages)\n",
    "\n",
    "# user can input a search keyword and the count of images required\n",
    "# download images from google search image\n",
    "Google_Image = \\\n",
    "    'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\n",
    "\n",
    "# The User-Agent request header contains a characteristic string \n",
    "# that allows the network protocol peers to identify the application type, \n",
    "# operating system, and software version of the requesting software user agent.\n",
    "# needed for google search\n",
    "u_agnt = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "    'Accept-Encoding': 'none',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Connection': 'keep-alive',\n",
    "} #write: 'my user agent' in browser to get your browser user agent details\n",
    "\n",
    "Image_Folder = 'Images_1'\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(Image_Folder):\n",
    "        os.mkdir(Image_Folder)\n",
    "    download_images()\n",
    "\n",
    "def download_images():\n",
    "    data = input('Enter your search keyword: ')\n",
    "    num_images = int(input('Enter the number of images you want: '))\n",
    "    \n",
    "    print('Searching Images....')\n",
    "    \n",
    "    search_url = Google_Image + 'q=' + data #'q=' because its a query\n",
    "    \n",
    "    # request url, without u_agnt the permission gets denied\n",
    "    response = requests.get(search_url, headers=u_agnt)\n",
    "    html = response.text #To get actual result i.e. to read the html data in text mode\n",
    "    \n",
    "    # find all img where class='rg_i Q4LuWd'\n",
    "    b_soup = BeautifulSoup(html, 'html.parser') #html.parser is used to parse/extract features from HTML files\n",
    "    results = b_soup.findAll('img', {'class': 'rg_i Q4LuWd'})\n",
    "    \n",
    "    #extract the links of requested number of images with 'data-src' attribute and appended those links to a list 'imagelinks'\n",
    "    #allow to continue the loop in case query fails for non-data-src attributes\n",
    "    count = 0\n",
    "    imagelinks= []\n",
    "    urls=[]\n",
    "    for res in results:\n",
    "        try:\n",
    "            link = res['data-src']\n",
    "            imagelinks.append(link)\n",
    "            urls.append(link)\n",
    "            count = count + 1\n",
    "            if (count >= num_images):\n",
    "                break\n",
    "            \n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    print(f'Found {len(imagelinks)} images')\n",
    "    print('Start downloading...')\n",
    "\n",
    "    for i, imagelink in enumerate(imagelinks):\n",
    "        # open each image link and save the file\n",
    "        response = requests.get(imagelink)\n",
    "        imagename = Image_Folder + '/' + data + str(i+1) + '.jpg'\n",
    "        with open(imagename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            \n",
    "    # Name of the SQL Database is image_urls        \n",
    "    conn=sqlite3.connect('image_urls.db')\n",
    "    cursor=conn.cursor()\n",
    "    cursor.execute('CREATE TABLE IF NOT EXISTS image_urls (id INTEGER PRIMARY KEY AUTOINCREMENT, url TEXT)')\n",
    "    \n",
    "    for url in urls:\n",
    "        cursor.execute('INSERT INTO image_urls (url) VALUES (?)', (url,))\n",
    "        \n",
    "    cursor.execute('SELECT * FROM image_urls ORDER BY url DESC LIMIT 4')\n",
    "    rows=cursor.fetchmany(4)\n",
    "    \n",
    "    for row in rows:\n",
    "        print(row)\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print('Download Completed!')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d3d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fc1f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2827913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the URL\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89db496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae9eb6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the image \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Send a GET request to the URL\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[43mimage_url\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Get the content of the response\u001b[39;00m\n\u001b[1;32m      7\u001b[0m image_content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_url' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the image \n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(image_url)\n",
    "\n",
    "# Get the content of the response\n",
    "image_content = response.content\n",
    "\n",
    "# Define the file name to save the image as\n",
    "file_name = 'product_image.jpg'\n",
    "\n",
    "# Open the file in binary mode and write the image content to it\n",
    "with open(file_name, 'wb') as f:\n",
    "    f.write(image_content)\n",
    "\n",
    "print('Image saved successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3e936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ea9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
